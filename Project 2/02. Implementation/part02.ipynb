{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Cài đặt thuật toán tiến trước, thuật toán Viterbi, và thuật toán Baum-Welch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Thuật toán tiến trước"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://github.com/adeveloperdiary/HiddenMarkovModel/blob/master/part2/forward.py](https://github.com/adeveloperdiary/HiddenMarkovModel/blob/master/part2/forward.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' The forward algorithm: \n",
    "Given an HMM with:\n",
    "transition probability table A and emission probability table (observation ikelihood) B \n",
    "return the probability (likelihood) of a observation sequence O'''\n",
    "\n",
    "# all the variables in this function are np.arrays\n",
    "def forward_algorithm(observation, transition_prob, emission_prob, initial_distribution, vocabulary):\n",
    "    'observations has length of T, the number of different states N'\n",
    "    # create a probability matrix alpha\n",
    "    T = observation.shape[0]\n",
    "    N = transition_prob.shape[0]\n",
    "    alpha = np.zeros((N, T))\n",
    "    \n",
    "    # initialization\n",
    "    id = np.where(vocabulary == observation[0]) # get the index of the first observation in the vocabulary\n",
    "\n",
    "    alpha[: , 0] = initial_distribution * emission_prob[:, id[0][0]]\n",
    "\n",
    "    # recursion\n",
    "    for i in range(1, T):\n",
    "        id = np.where(vocabulary == observation[i])\n",
    "        for j in range(N):\n",
    "            alpha[j, i] = alpha[:, i - 1].dot(transition_prob[:, j]) * emission_prob[j, id[0][0]]\n",
    "\n",
    "    # termination\n",
    "    forward_prob = np.sum(alpha[:, T - 1])\n",
    "    return (forward_prob, alpha)\n",
    "\n",
    "\n",
    "# obs = np.array((3, 1, 3))\n",
    "# trans = np.array([[.5, .5], [.4, .6]])\n",
    "# ems = np.array([[.5, .4, .1], [.2, .4, .4]])\n",
    "# ini = np.array((.2, .8))\n",
    "# vcb = np.array((1, 2, 3))\n",
    "# print(forward_algorithm(obs, trans, ems, ini, vcb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Thuật toán Viterbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://github.com/adeveloperdiary/HiddenMarkovModel/blob/master/part4/Viterbi.py](https://github.com/adeveloperdiary/HiddenMarkovModel/blob/master/part4/Viterbi.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Viterbi algorithm:\n",
    "Given an HMM with:\n",
    "transition probability table A and emission probability table (observation ikelihood) B \n",
    "a sequence of observations O\n",
    "find the most sequence of states Q\n",
    "'''\n",
    "def viterbi_algorithm(observation, transition_prob, emission_prob, initial_distribution, vocabulary):\n",
    "    T = observation.shape[0]\n",
    "    N = transition_prob.shape[0]\n",
    "    omega = np.zeros((N, T))\n",
    "    \n",
    "    # initialization\n",
    "    id = np.where(vocabulary == observation[0])\n",
    "    \n",
    "    omega[:, 0] = initial_distribution * emission_prob[:, id[0][0]] # initialize the same as the forward algorithm\n",
    "\n",
    "    prev = np.zeros((N, T))\n",
    "    prev[:, 0] = 0\n",
    "\n",
    "    temp = np.zeros(N)\n",
    "\n",
    "    # recursion\n",
    "    for i in range(1, T):\n",
    "        # find the index of the observation in the vocabulary\n",
    "        id = np.where(vocabulary == observation[i])\n",
    "        for j in range(N):    \n",
    "            # the same as forward probability\n",
    "            for k in range(N):\n",
    "                temp[k] = omega[k, i - 1] * transition_prob[k, j] * emission_prob[j, id[0][0]]\n",
    "                \n",
    "            # the most probable state given previous state at time i    (1)\n",
    "            prev[j, i] = np.argmax(temp)\n",
    "\n",
    "            # the probability of the most probable state                (2)\n",
    "            omega[j, i] = np.max(temp)\n",
    "\n",
    "    # print(omega)\n",
    "\n",
    "    # termination\n",
    "    best_path_prob = np.max(omega[:, T - 1])\n",
    "\n",
    "    # path array: the most probable sequence of states for the observations sequence.\n",
    "    path = np.zeros(T, dtype= int)\n",
    "\n",
    "    # the most probable state at the last time step\n",
    "    path[T - 1] = int(np.argmax(omega[:, T - 1]))\n",
    "\n",
    "    # backtracking\n",
    "    for i in range(T - 2, -1, -1):\n",
    "        path[i] = int(prev[path[i + 1], i + 1])\n",
    "\n",
    "    return (path)\n",
    "\n",
    "# print(viterbi_algorithm(obs, trans, ems, ini, vcb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Thuật toán Baum - Welch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward probability\n",
    "def backward_algorithm(observation, transition_prob, emission_prob, initial_distribution, vocabulary):\n",
    "    # initialization\n",
    "    T = observation.shape[0]\n",
    "    N = transition_prob.shape[0]\n",
    "    beta = np.zeros((N, T))\n",
    "\n",
    "    # initialization\n",
    "    for i in range(N):\n",
    "        beta[i, T - 1] = 1\n",
    "    \n",
    "    # recursion\n",
    "    for t in range(T - 2, -1, -1):\n",
    "        for i in range(N):\n",
    "            # find the index of the (t + 1)-th observation\n",
    "            id = np.where(vocabulary == observation[t + 1])\n",
    "            beta[i, t] = 0\n",
    "\n",
    "            for j in range(N):\n",
    "                beta[i, t] = beta[i, t] + transition_prob[i, j] * emission_prob[j, id[0][0]] * beta[j, t + 1]\n",
    "    \n",
    "    # termination\n",
    "    id = np.where(vocabulary == observation[0])\n",
    "    print(initial_distribution.shape, emission_prob.shape, beta[:, 0].shape)\n",
    "    backward_prob = np.dot(np.dot(initial_distribution , emission_prob[:, id[0][0]]), beta[:, 0])\n",
    "    \n",
    "    return (backward_prob, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baum-welch algorithm: from the observation sequence O and the set of possible states in the HMM, learn the HMM parameters A (alpha) and B (beta)\n",
    "def baum_welch_algorithm(observation, transition_prob, emission_prob, initial_distribution, vocabulary, n_iter=100):\n",
    "    T = observation.shape[0]\n",
    "    N = transition_prob.shape[0]\n",
    "    \n",
    "    for n in range(n_iter):\n",
    "        (f_pr, alpha) = forward_algorithm(observation, transition_prob, emission_prob, initial_distribution, vocabulary)\n",
    "        (b_pr, beta) = backward_algorithm(observation, transition_prob, emission_prob, initial_distribution, vocabulary)\n",
    "        xi = np.zeros((N, N, T - 1))\n",
    "\n",
    "        for t in range(T - 1):\n",
    "            id = np.where(vocabulary == observation[t + 1])\n",
    "\n",
    "            denominator = np.dot(np.dot(alpha[ : , t], transition_prob) * emission_prob[:, id[0][0]], beta[:, t + 1])\n",
    "            for i in range(N):\n",
    "                numerator = alpha[i, t] * transition_prob[i, :] * emission_prob[:, id[0][0]] * beta[:, t + 1]\n",
    "                xi[i, :, t] = numerator / denominator\n",
    "\n",
    "        gamma = np.sum(xi, axis=1)\n",
    "        alpha = np.sum(xi, 2) / np.sum(gamma, axis=1).reshape((-1, 1))\n",
    "\n",
    "        # Add additional T'th element in gamma\n",
    "        gamma = np.hstack((gamma, np.sum(xi[:, :, T - 2], axis=0).reshape((-1, 1))))\n",
    "\n",
    "        K = emission_prob.shape[1]\n",
    "        denominator = np.sum(gamma, axis=1)\n",
    "        for l in range(K):\n",
    "            emission_prob[:, l] = np.sum(gamma[:, observation == l], axis=1)\n",
    "\n",
    "        emission_prob = np.divide(emission_prob, denominator.reshape((-1, 1)))\n",
    "\n",
    "    return (transition_prob, emission_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Bài toán: \n",
    "Khi làm quản trò, anh Huy thường sử dụng 2 viên xúc xác khác nhau. Viên đầu tiên là một viên xúc xắc cân bằng, mọi mặt đều có cùng xác suất. Viên thứ hai là một viên xúc xắc lỗi, khi tung sẽ có 50% xác suất ra mặt số 6 và 10% xác suất ra mỗi mặt còn lại. Mỗi lần tung, anh sẽ chọn 1 trong 2 viên xúc xắc này để tung. Người chơi không thể biết anh đã tung viên nào, chỉ biết được lần tung đó ra mặt nào. Ngoài ra, nếu ở lần tung này, anh Huy sử dụng viên xúc xắc cân bằng, thì có 80% khả năng anh sẽ tiếp tục sử dụng viên xúc xắc này cho lần tung tiếp theo (20% còn lại anh sẽ đổi sang dùng viên lỗi). Con số này là 30% đối với viên lỗi (70% đổi sang dùng viên cân bằng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Mô hình hóa tình huống trên bằng một mô hình Markov ẩn. Cho biết các tham số của mô hình này.\n",
    "Mô hình Markov ẩn được xây dựng:\n",
    "- Tập trạng thái `Q = {q0: cân bằng, q1: lỗi}`.\n",
    "- Ma trận chuyển trạng thái `A = [[0.8, 0.2], [0.7, 0.3]]` (theo thứ tự `a00, a01, a10, a11`).\n",
    "- Tập quan sát O gồm các trạng thái được lấy từ tập `V = {1, 2, 3, 4, 5, 6}`.\n",
    "- Ma trận B (các giá trị observation likelihoods): \n",
    "`B = [[P(1|q0) = 1/6, P(2|q0) = 1/6, P(3|q0) = 1/6, P(4|q0) = 1/6, P(5|q0) = 1/6, P(6|q0) = 1/6],\n",
    " [P(1|q1) = 0.1, P(2|q1) = 0.1, P(3|q1) = 0.1, P(4|q1) = 0.1, P(5|q1) = 0.1, P(6|q1) = 0.5]]`\n",
    "- Phân phối ban đầu `Pi = [0.5, 0.5]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Sinh ngẫu nhiên một chuỗi T = 100 lần tung theo đúng mô tả trên."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation sequence:  [4 5 6 1 6 6 6 4 5 6 6 5 6 3 1 1 6 2 3 6 3 4 1 4 2 4 4 6 6 6 2 6 2 3 1 3 1\n",
      " 1 6 3 3 5 3 6 4 2 6 1 4 4 1 5 2 2 3 6 3 5 4 3 3 6 5 5 6 5 1 6 1 6 6 1 3 4\n",
      " 5 4 5 6 4 2 3 4 4 6 6 4 5 1 2 6 6 6 6 2 2 5 3 1 2 2]\n"
     ]
    }
   ],
   "source": [
    "'A is the transition probability matrix'\n",
    "A = [[0.8, 0.2], [0.7, 0.3]]\n",
    "\n",
    "'B is the observation likelihoods matrix'\n",
    "B = [[1/6, 1/6, 1/6, 1/6, 1/6, 1/6],[.1, .1, .1, .1, .1, .5]]\n",
    "\n",
    "'D is the number on the faces of the dice'\n",
    "D = [1,2,3,4,5,6]\n",
    "\n",
    "'initial distribution'\n",
    "Pi = np.array((0.5, 0.5))\n",
    "\n",
    "def generate(T: int):\n",
    "    dice = rd.choice([0,1])     # choose a random dice with equal probability 0.5\n",
    "    res = list()                # list of observations\n",
    "    dices = list()              # hidden states\n",
    "    for i in range(0, T):\n",
    "        dices.append(dice)\n",
    "        temp1 = rd.choices(D, B[dice])\n",
    "        res.append(temp1[0])\n",
    "        temp2 = rd.choices([0,1], A[dice])\n",
    "        dice = temp2[0]\n",
    "    # print('Hidden states: ', dices)\n",
    "    return res\n",
    "\n",
    "'Generate a sequence of T = 100 observations'\n",
    "obs = np.array(generate(100))   # observation seq.\n",
    "trans = np.array(A)  # transition prob.\n",
    "emiss = np.array(B)  # emission prob.\n",
    "vocab = np.array(D)  # vocabulary\n",
    "print('Observation sequence: ', obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Sử dụng thuật toán Viterbi để dự đoán viên xúc xắc được dùng cho mỗi lần tung. Độ chính xác của dự đoán này là bao nhiêu? Hãy lặp lại thí nghiệm này nhiều lần nếu cần thiết. Báo cáo và nhận xét kết quả thu được."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "state_guess = viterbi_algorithm(obs, trans, emiss, Pi, vocab)\n",
    "print(state_guess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Giả sử bạn là một người chơi, hãy sử dụng thuật toán Baum-Welch để ước lượng các tham số cho mô hình Markov ẩn. Hãy lặp lại thí nghiệm nhiều lần nếu cần thiết. Báo cáo và nhận xét kết quả thu được."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp/ipykernel_14400/478258657.py:17: RuntimeWarning: invalid value encountered in true_divide\n",
      "  xi[i, :, t] = numerator / denominator\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "(2,) (2, 6) (2,)\n",
      "alpha =  [[1. 0.]\n",
      " [0. 1.]]\n",
      "beta =  [[nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    "# transition probabilities\n",
    "alpha = np.identity(2)\n",
    "\n",
    "# emission probabilities\n",
    "beta = np.ones((2, 6))/6\n",
    "\n",
    "alpha, beta = baum_welch_algorithm(obs, alpha, beta, Pi, vocab)\n",
    "print('alpha = ', alpha)\n",
    "print('beta = ', beta)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ae58900cfbb8c43ab3495913814b7cf26024f51651a94ce8bf64d6111688e8d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
